#include <arrow/api.h>
#include <arrow/flight/api.h>
#include <arrow/io/api.h>
#include <arrow/ipc/api.h>
#include <arrow/result.h>
#include <arrow/status.h>
#include <arrow/table.h>

#include <iostream>
#include <memory>
#include <string>
#include <unordered_map>
#include <mutex>
#include <thread>
#include <chrono>
#include <sstream>

namespace flight = arrow::flight;
namespace fs = arrow::fs;

// Data storage structure for the relay server
struct StoredTable {
    std::shared_ptr<arrow::Table> table;
    std::string description;
    std::chrono::system_clock::time_point timestamp;
    std::string sender_id;
    size_t access_count;
    
    StoredTable(std::shared_ptr<arrow::Table> t, const std::string& desc, 
                const std::string& sender)
        : table(std::move(t)), description(desc), 
          timestamp(std::chrono::system_clock::now()),
          sender_id(sender), access_count(0) {}
};

class ArrowFlightRelayServer : public flight::FlightServerBase {
private:
    std::unordered_map<std::string, std::unique_ptr<StoredTable>> data_store_;
    mutable std::mutex data_mutex_;
    size_t max_stored_tables_;
    bool enable_logging_;

public:
    explicit ArrowFlightRelayServer(size_t max_tables = 1000, bool logging = true)
        : max_stored_tables_(max_tables), enable_logging_(logging) {
        if (enable_logging_) {
            std::cout << "[Server] ArrowFlight Relay Server initialized" << std::endl;
            std::cout << "[Server] Maximum stored tables: " << max_stored_tables_ << std::endl;
        }
    }

    // List all available flights (stored tables)
    arrow::Status ListFlights(const flight::ServerCallContext& context,
                             const flight::Criteria* criteria,
                             std::unique_ptr<flight::FlightListing>* listings) override {
        std::lock_guard<std::mutex> lock(data_mutex_);
        
        std::vector<flight::FlightInfo> flights;
        
        for (const auto& [key, stored_table] : data_store_) {
            // Create flight descriptor
            auto descriptor = flight::FlightDescriptor::Path({key});
            
            // Create flight endpoint
            flight::Location location;
            ARROW_ASSIGN_OR_RAISE(location, flight::Location::ForGrpcTcp("localhost", 0));
            std::vector<flight::FlightEndpoint> endpoints{flight::FlightEndpoint{{"localhost:8080"}, {location}}};
            
            // Get table schema
            auto schema = stored_table->table->schema();
            
            // Create flight info with metadata
            auto info = flight::FlightInfo::Make(*schema, descriptor, endpoints, 
                                               stored_table->table->num_rows(), 
                                               -1);  // -1 for unknown size
            
            flights.push_back(*info);
        }
        
        *listings = std::make_unique<flight::SimpleFlightListing>(flights);
        
        if (enable_logging_) {
            std::cout << "[Server] Listed " << flights.size() << " available flights" << std::endl;
        }
        
        return arrow::Status::OK();
    }

    // Get flight info for a specific table
    arrow::Status GetFlightInfo(const flight::ServerCallContext& context,
                               const flight::FlightDescriptor& descriptor,
                               std::unique_ptr<flight::FlightInfo>* info) override {
        if (descriptor.type != flight::FlightDescriptor::PATH || descriptor.path.empty()) {
            return arrow::Status::Invalid("Invalid flight descriptor");
        }
        
        std::string table_key = descriptor.path[0];
        std::lock_guard<std::mutex> lock(data_mutex_);
        
        auto it = data_store_.find(table_key);
        if (it == data_store_.end()) {
            return arrow::Status::KeyError("Table not found: " + table_key);
        }
        
        // Create flight endpoint
        flight::Location location;
        ARROW_ASSIGN_OR_RAISE(location, flight::Location::ForGrpcTcp("localhost", 8080));
        std::vector<flight::FlightEndpoint> endpoints{flight::FlightEndpoint{{table_key}, {location}}};
        
        auto flight_info = flight::FlightInfo::Make(
            *it->second->table->schema(),
            descriptor,
            endpoints,
            it->second->table->num_rows(),
            -1
        );
        
        *info = std::make_unique<flight::FlightInfo>(*flight_info);
        
        if (enable_logging_) {
            std::cout << "[Server] Flight info requested for: " << table_key << std::endl;
        }
        
        return arrow::Status::OK();
    }

    // Serve data to clients (DoGet)
    arrow::Status DoGet(const flight::ServerCallContext& context,
                       const flight::Ticket& ticket,
                       std::unique_ptr<flight::FlightDataStream>* stream) override {
        std::string table_key = ticket.ticket;
        
        std::lock_guard<std::mutex> lock(data_mutex_);
        auto it = data_store_.find(table_key);
        if (it == data_store_.end()) {
            return arrow::Status::KeyError("Table not found: " + table_key);
        }
        
        // Increment access count
        it->second->access_count++;
        
        // Create a record batch reader from the table
        auto reader = std::make_shared<arrow::TableBatchReader>(*it->second->table);
        
        // Create flight data stream
        *stream = std::make_unique<flight::RecordBatchStream>(reader);
        
        if (enable_logging_) {
            std::cout << "[Server] Serving table: " << table_key 
                     << " (access count: " << it->second->access_count << ")" << std::endl;
        }
        
        return arrow::Status::OK();
    }

    // Receive data from clients (DoPut)
    arrow::Status DoPut(const flight::ServerCallContext& context,
                       std::unique_ptr<flight::FlightMessageReader> reader,
                       std::unique_ptr<flight::FlightMetadataWriter> writer) override {
        
        // Get the flight descriptor from the first message
        flight::FlightDescriptor descriptor;
        ARROW_RETURN_NOT_OK(reader->GetDescriptor(&descriptor));
        
        if (descriptor.type != flight::FlightDescriptor::PATH || descriptor.path.empty()) {
            return arrow::Status::Invalid("Invalid flight descriptor for put operation");
        }
        
        std::string table_key = descriptor.path[0];
        
        // Read all record batches
        std::vector<std::shared_ptr<arrow::RecordBatch>> batches;
        while (true) {
            flight::FlightStreamChunk chunk;
            ARROW_RETURN_NOT_OK(reader->Next(&chunk));
            if (chunk.data == nullptr) break;
            batches.push_back(chunk.data);
        }
        
        if (batches.empty()) {
            return arrow::Status::Invalid("No data received");
        }
        
        // Convert batches to table
        ARROW_ASSIGN_OR_RAISE(auto table, arrow::Table::FromRecordBatches(batches));
        
        // Extract metadata if available
        std::string description = "Data table";
        std::string sender_id = "unknown";
        
        // Try to get metadata from context
        auto metadata = context.incoming_headers();
        for (const auto& [key, value] : metadata) {
            if (key == "description") {
                description = value;
            } else if (key == "sender_id") {
                sender_id = value;
            }
        }
        
        // Store the table
        {
            std::lock_guard<std::mutex> lock(data_mutex_);
            
            // Check if we're at capacity and need to remove old entries
            if (data_store_.size() >= max_stored_tables_) {
                // Find oldest entry to remove
                auto oldest = data_store_.begin();
                for (auto it = data_store_.begin(); it != data_store_.end(); ++it) {
                    if (it->second->timestamp < oldest->second->timestamp) {
                        oldest = it;
                    }
                }
                if (enable_logging_) {
                    std::cout << "[Server] Removing oldest table: " << oldest->first << std::endl;
                }
                data_store_.erase(oldest);
            }
            
            data_store_[table_key] = std::make_unique<StoredTable>(table, description, sender_id);
        }
        
        if (enable_logging_) {
            std::cout << "[Server] Stored table: " << table_key 
                     << " (" << table->num_rows() << " rows, " 
                     << table->num_columns() << " columns)" 
                     << " from sender: " << sender_id << std::endl;
        }
        
        return arrow::Status::OK();
    }

    // Get server statistics
    arrow::Status DoAction(const flight::ServerCallContext& context,
                          const flight::Action& action,
                          std::unique_ptr<flight::ResultStream>* result) override {
        
        if (action.type == "get_stats") {
            std::lock_guard<std::mutex> lock(data_mutex_);
            
            std::stringstream stats;
            stats << "{\n";
            stats << "  \"stored_tables\": " << data_store_.size() << ",\n";
            stats << "  \"max_capacity\": " << max_stored_tables_ << ",\n";
            stats << "  \"tables\": [\n";
            
            bool first = true;
            for (const auto& [key, stored_table] : data_store_) {
                if (!first) stats << ",\n";
                first = false;
                
                auto duration = std::chrono::duration_cast<std::chrono::seconds>(
                    std::chrono::system_clock::now() - stored_table->timestamp
                ).count();
                
                stats << "    {\n";
                stats << "      \"key\": \"" << key << "\",\n";
                stats << "      \"rows\": " << stored_table->table->num_rows() << ",\n";
                stats << "      \"columns\": " << stored_table->table->num_columns() << ",\n";
                stats << "      \"age_seconds\": " << duration << ",\n";
                stats << "      \"access_count\": " << stored_table->access_count << ",\n";
                stats << "      \"sender_id\": \"" << stored_table->sender_id << "\",\n";
                stats << "      \"description\": \"" << stored_table->description << "\"\n";
                stats << "    }";
            }
            
            stats << "\n  ]\n}";
            
            auto buffer = arrow::Buffer::FromString(stats.str());
            std::vector<std::shared_ptr<arrow::Buffer>> results = {buffer};
            *result = std::make_unique<flight::SimpleResultStream>(std::move(results));
            
            if (enable_logging_) {
                std::cout << "[Server] Statistics requested" << std::endl;
            }
            
            return arrow::Status::OK();
        }
        
        if (action.type == "clear_all") {
            std::lock_guard<std::mutex> lock(data_mutex_);
            size_t count = data_store_.size();
            data_store_.clear();
            
            std::string response = "Cleared " + std::to_string(count) + " tables";
            auto buffer = arrow::Buffer::FromString(response);
            std::vector<std::shared_ptr<arrow::Buffer>> results = {buffer};
            *result = std::make_unique<flight::SimpleResultStream>(std::move(results));
            
            if (enable_logging_) {
                std::cout << "[Server] Cleared all stored tables (" << count << ")" << std::endl;
            }
            
            return arrow::Status::OK();
        }
        
        return arrow::Status::NotImplemented("Unknown action: " + action.type);
    }

    // List available actions
    arrow::Status ListActions(const flight::ServerCallContext& context,
                             std::vector<flight::ActionType>* actions) override {
        *actions = {
            {"get_stats", "Get server statistics and stored table information"},
            {"clear_all", "Clear all stored tables"}
        };
        return arrow::Status::OK();
    }
};

// Main server function
int main(int argc, char** argv) {
    // Parse command line arguments
    int port = 8080;
    size_t max_tables = 1000;
    bool enable_logging = true;
    
    if (argc > 1) {
        port = std::atoi(argv[1]);
    }
    if (argc > 2) {
        max_tables = std::atoi(argv[2]);
    }
    if (argc > 3) {
        enable_logging = (std::string(argv[3]) == "true");
    }
    
    // Create and configure server
    auto server = std::make_unique<ArrowFlightRelayServer>(max_tables, enable_logging);
    
    flight::Location location;
    auto location_result = flight::Location::ForGrpcTcp("0.0.0.0", port);
    if (!location_result.ok()) {
        std::cerr << "Failed to create location: " << location_result.status().ToString() << std::endl;
        return 1;
    }
    location = location_result.ValueOrDie();
    
    flight::FlightServerOptions options(location);
    
    std::cout << "Starting ArrowFlight Relay Server on " << location.ToString() << std::endl;
    std::cout << "Maximum stored tables: " << max_tables << std::endl;
    std::cout << "Logging enabled: " << (enable_logging ? "true" : "false") << std::endl;
    std::cout << "Usage: " << argv[0] << " [port] [max_tables] [enable_logging]" << std::endl;
    std::cout << "Press Ctrl+C to stop the server" << std::endl;
    
    // Start server
    auto init_result = server->Init(options);
    if (!init_result.ok()) {
        std::cerr << "Failed to start server: " << init_result.ToString() << std::endl;
        return 1;
    }
    
    auto serve_result = server->Serve();
    if (!serve_result.ok()) {
        std::cerr << "Server error: " << serve_result.ToString() << std::endl;
        return 1;
    }
    
    return 0;
}

#!/usr/bin/env python3
"""
ArrowFlight Python Client for communicating with the C++ relay server.
Supports sending and receiving pandas DataFrames and Arrow tables.
"""

import pyarrow as pa
import pyarrow.flight as flight
import pandas as pd
import json
import uuid
import time
from typing import Dict, List, Optional, Union, Any, Tuple
from contextlib import contextmanager
import logging


class ArrowFlightClient:
    """
    Python client for the ArrowFlight relay server.
    Handles conversion between pandas DataFrames and Arrow tables.
    """
    
    def __init__(self, host: str = "localhost", port: int = 8080, 
                 sender_id: Optional[str] = None, timeout: float = 30.0):
        """
        Initialize the ArrowFlight client.
        
        Args:
            host: Server hostname
            port: Server port
            sender_id: Unique identifier for this client
            timeout: Connection timeout in seconds
        """
        self.host = host
        self.port = port
        self.sender_id = sender_id or f"client_{uuid.uuid4().hex[:8]}"
        self.timeout = timeout
        
        # Create flight client
        location = flight.Location.for_grpc_tcp(host, port)
        self.client = flight.FlightClient(location)
        
        # Setup logging
        self.logger = logging.getLogger(f"ArrowFlightClient-{self.sender_id}")
        
        # Test connection
        self._test_connection()
    
    def _test_connection(self) -> None:
        """Test connection to the server."""
        try:
            list(self.client.list_flights())
            self.logger.info(f"Connected to ArrowFlight server at {self.host}:{self.port}")
        except Exception as e:
            raise ConnectionError(f"Failed to connect to server: {e}")
    
    @contextmanager
    def _handle_flight_errors(self, operation: str):
        """Context manager for handling Flight errors with meaningful messages."""
        try:
            yield
        except flight.FlightError as e:
            self.logger.error(f"{operation} failed: {e}")
            raise
        except Exception as e:
            self.logger.error(f"Unexpected error during {operation}: {e}")
            raise
    
    def send_dataframe(self, key: str, df: pd.DataFrame, 
                      description: str = "DataFrame from Python") -> bool:
        """
        Send a pandas DataFrame to the server.
        
        Args:
            key: Unique identifier for the data
            df: pandas DataFrame to send
            description: Description of the data
            
        Returns:
            True if successful, False otherwise
        """
        if df.empty:
            self.logger.warning("Attempting to send empty DataFrame")
            return False
        
        try:
            # Convert DataFrame to Arrow table
            table = pa.Table.from_pandas(df)
            return self.send_table(key, table, description)
        except Exception as e:
            self.logger.error(f"Failed to convert DataFrame to Arrow table: {e}")
            return False
    
    def send_table(self, key: str, table: pa.Table, 
                   description: str = "Arrow table from Python") -> bool:
        """
        Send an Arrow table to the server.
        
        Args:
            key: Unique identifier for the data
            table: Arrow table to send
            description: Description of the data
            
        Returns:
            True if successful, False otherwise
        """
        with self._handle_flight_errors(f"sending table '{key}'"):
            # Create flight descriptor
            descriptor = flight.FlightDescriptor.for_path(key)
            
            # Create metadata headers
            headers = [
                (b"description", description.encode()),
                (b"sender_id", self.sender_id.encode())
            ]
            
            # Convert table to record batches
            batches = table.to_batches()
            
            # Send data to server
            writer, _ = self.client.do_put(descriptor, table.schema, headers)
            
            with writer:
                for batch in batches:
                    writer.write_batch(batch)
            
            self.logger.info(f"Successfully sent table '{key}' "
                           f"({table.num_rows} rows, {table.num_columns} columns)")
            return True
    
    def get_dataframe(self, key: str) -> Optional[pd.DataFrame]:
        """
        Retrieve a pandas DataFrame from the server.
        
        Args:
            key: Identifier of the data to retrieve
            
        Returns:
            pandas DataFrame or None if not found
        """
        table = self.get_table(key)
        if table is None:
            return None
        
        try:
            df = table.to_pandas()
            self.logger.info(f"Retrieved DataFrame '{key}' "
                           f"({len(df)} rows, {len(df.columns)} columns)")
            return df
        except Exception as e:
            self.logger.error(f"Failed to convert Arrow table to DataFrame: {e}")
            return None
    
    def get_table(self, key: str) -> Optional[pa.Table]:
        """
        Retrieve an Arrow table from the server.
        
        Args:
            key: Identifier of the data to retrieve
            
        Returns:
            Arrow table or None if not found
        """
        with self._handle_flight_errors(f"retrieving table '{key}'"):
            # Create ticket for the data
            ticket = flight.Ticket(key.encode())
            
            # Get data stream
            reader = self.client.do_get(ticket)
            
            # Read all batches and convert to table
            batches = []
            for chunk in reader:
                batches.append(chunk.data)
            
            if not batches:
                self.logger.warning(f"No data found for key '{key}'")
                return None
            
            table = pa.Table.from_batches(batches)
            self.logger.info(f"Retrieved table '{key}' "
                           f"({table.num_rows} rows, {table.num_columns} columns)")
            return table
    
    def list_tables(self) -> List[Dict[str, Any]]:
        """
        List all available tables on the server.
        
        Returns:
            List of dictionaries with table information
        """
        with self._handle_flight_errors("listing tables"):
            flights = list(self.client.list_flights())
            
            tables = []
            for flight_info in flights:
                if flight_info.descriptor.path:
                    table_info = {
                        'key': flight_info.descriptor.path[0],
                        'schema': flight_info.schema,
                        'total_records': flight_info.total_records,
                        'total_bytes': flight_info.total_bytes,
                        'endpoints': len(flight_info.endpoints)
                    }
                    tables.append(table_info)
            
            self.logger.info(f"Found {len(tables)} tables on server")
            return tables
    
    def get_server_stats(self) -> Dict[str, Any]:
        """
        Get server statistics and information about stored tables.
        
        Returns:
            Dictionary with server statistics
        """
        with self._handle_flight_errors("getting server statistics"):
            action = flight.Action("get_stats", b"")
            results = list(self.client.do_action(action))
            
            if results:
                stats_json = results[0].body.to_pybytes().decode()
                return json.loads(stats_json)
            
            return {}
    
    def clear_server(self) -> bool:
        """
        Clear all stored tables from the server.
        
        Returns:
            True if successful, False otherwise
        """
        with self._handle_flight_errors("clearing server"):
            action = flight.Action("clear_all", b"")
            results = list(self.client.do_action(action))
            
            if results:
                response = results[0].body.to_pybytes().decode()
                self.logger.info(f"Server response: {response}")
                return True
            
            return False
    
    def table_exists(self, key: str) -> bool:
        """
        Check if a table exists on the server.
        
        Args:
            key: Table identifier to check
            
        Returns:
            True if table exists, False otherwise
        """
        try:
            descriptor = flight.FlightDescriptor.for_path(key)
            self.client.get_flight_info(descriptor)
            return True
        except flight.FlightError:
            return False
    
    def wait_for_table(self, key: str, timeout: float = 30.0, 
                      poll_interval: float = 0.5) -> bool:
        """
        Wait for a table to become available on the server.
        
        Args:
            key: Table identifier to wait for
            timeout: Maximum time to wait in seconds
            poll_interval: Time between checks in seconds
            
        Returns:
            True if table becomes available, False if timeout
        """
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            if self.table_exists(key):
                self.logger.info(f"Table '{key}' is now available")
                return True
            
            time.sleep(poll_interval)
        
        self.logger.warning(f"Timeout waiting for table '{key}'")
        return False
    
    def get_table_info(self, key: str) -> Optional[Dict[str, Any]]:
        """
        Get detailed information about a specific table.
        
        Args:
            key: Table identifier
            
        Returns:
            Dictionary with table information or None if not found
        """
        try:
            descriptor = flight.FlightDescriptor.for_path(key)
            flight_info = self.client.get_flight_info(descriptor)
            
            return {
                'key': key,
                'schema': flight_info.schema,
                'total_records': flight_info.total_records,
                'total_bytes': flight_info.total_bytes,
                'endpoints': len(flight_info.endpoints),
                'columns': [field.name for field in flight_info.schema],
                'column_types': [str(field.type) for field in flight_info.schema]
            }
        except flight.FlightError:
            return None


class ArrowFlightDataExchange:
    """
    High-level utility class for data exchange between Python processes
    using the ArrowFlight relay server.
    """
    
    def __init__(self, host: str = "localhost", port: int = 8080,
                 process_id: Optional[str] = None):
        """
        Initialize the data exchange utility.
        
        Args:
            host: Server hostname
            port: Server port
            process_id: Unique identifier for this process
        """
        self.process_id = process_id or f"process_{uuid.uuid4().hex[:8]}"
        self.client = ArrowFlightClient(host, port, sender_id=self.process_id)
        
        # Setup logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(f"DataExchange-{self.process_id}")
    
    def publish(self, channel: str, data: Union[pd.DataFrame, pa.Table],
               description: str = "Published data") -> bool:
        """
        Publish data to a named channel.
        
        Args:
            channel: Channel name
            data: Data to publish (DataFrame or Arrow table)
            description: Description of the data
            
        Returns:
            True if successful, False otherwise
        """
        if isinstance(data, pd.DataFrame):
            return self.client.send_dataframe(channel, data, description)
        elif isinstance(data, pa.Table):
            return self.client.send_table(channel, data, description)
        else:
            raise TypeError("Data must be pandas DataFrame or Arrow Table")
    
    def subscribe(self, channel: str, as_dataframe: bool = True) -> Optional[Union[pd.DataFrame, pa.Table]]:
        """
        Subscribe to data from a named channel.
        
        Args:
            channel: Channel name
            as_dataframe: If True, return pandas DataFrame; if False, return Arrow table
            
        Returns:
            Data from the channel or None if not available
        """
        if as_dataframe:
            return self.client.get_dataframe(channel)
        else:
            return self.client.get_table(channel)
    
    def wait_and_subscribe(self, channel: str, timeout: float = 30.0,
                          as_dataframe: bool = True) -> Optional[Union[pd.DataFrame, pa.Table]]:
        """
        Wait for data to become available on a channel and then subscribe.
        
        Args:
            channel: Channel name
            timeout: Maximum time to wait
            as_dataframe: If True, return pandas DataFrame; if False, return Arrow table
            
        Returns:
            Data from the channel or None if timeout
        """
        if self.client.wait_for_table(channel, timeout):
            return self.subscribe(channel, as_dataframe)
        return None
    
    def list_channels(self) -> List[str]:
        """
        List all available data channels.
        
        Returns:
            List of channel names
        """
        tables = self.client.list_tables()
        return [table['key'] for table in tables]
    
    def get_server_status(self) -> Dict[str, Any]:
        """
        Get server status and statistics.
        
        Returns:
            Dictionary with server information
        """
        return self.client.get_server_stats()


def main():
    """Example usage of the ArrowFlight client."""
    import numpy as np
    
    # Create sample data
    df = pd.DataFrame({
        'id': range(1000),
        'value': np.random.randn(1000),
        'category': np.random.choice(['A', 'B', 'C'], 1000),
        'timestamp': pd.date_range('2024-01-01', periods=1000, freq='1H')
    })
    
    try:
        # Initialize client
        print("🚀 Connecting to ArrowFlight server...")
        client = ArrowFlightClient()
        
        # Send DataFrame
        print("📤 Sending sample DataFrame...")
        success = client.send_dataframe(
            "sample_data", 
            df, 
            "Sample data with 1000 rows"
        )
        print(f"   Send result: {'✅ Success' if success else '❌ Failed'}")
        
        # List tables
        print("\n📋 Available tables:")
        tables = client.list_tables()
        for table in tables:
            print(f"   • {table['key']}: {table['total_records']} records")
        
        # Retrieve DataFrame
        print("\n📥 Retrieving DataFrame...")
        retrieved_df = client.get_dataframe("sample_data")
        if retrieved_df is not None:
            print(f"   ✅ Retrieved DataFrame: {len(retrieved_df)} rows, {len(retrieved_df.columns)} columns")
            print(f"   Sample data:\n{retrieved_df.head()}")
        else:
            print("   ❌ Failed to retrieve DataFrame")
        
        # Get server stats
        print("\n📊 Server statistics:")
        stats = client.get_server_stats()
        print(f"   Stored tables: {stats.get('stored_tables', 'unknown')}")
        print(f"   Max capacity: {stats.get('max_capacity', 'unknown')}")
        
        # Example of high-level data exchange
        print("\n🔄 Testing high-level data exchange...")
        exchange = ArrowFlightDataExchange()
        
        # Publish data
        sample_data = pd.DataFrame({
            'x': np.random.randn(100),
            'y': np.random.randn(100)
        })
        
        exchange.publish("test_channel", sample_data, "Test data for exchange")
        
        # Subscribe to data
        received_data = exchange.subscribe("test_channel")
        if received_data is not None:
            print(f"   ✅ Data exchange successful: {len(received_data)} rows")
        else:
            print("   ❌ Data exchange failed")
        
        print("\n🎉 All tests completed!")
        
    except Exception as e:
        print(f"❌ Error: {e}")


if __name__ == "__main__":
    main()

cmake_minimum_required(VERSION 3.16)
project(ArrowFlightRelayServer)

# Set C++ standard
set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# Find required packages
find_package(Arrow REQUIRED)
find_package(ArrowFlight REQUIRED)
find_package(Threads REQUIRED)

# Include directories
include_directories(${ARROW_INCLUDE_DIR})

# Add executable
add_executable(arrow_flight_server
    arrow_flight_server.cpp
)

# Link libraries
target_link_libraries(arrow_flight_server
    PRIVATE
    Arrow::arrow_shared
    ArrowFlight::arrow_flight_shared
    Threads::Threads
)

# Compiler-specific options
if(CMAKE_CXX_COMPILER_ID STREQUAL "GNU" OR CMAKE_CXX_COMPILER_ID STREQUAL "Clang")
    target_compile_options(arrow_flight_server PRIVATE
        -Wall
        -Wextra
        -Wpedantic
        -O3
    )
endif()

# Set output directory
set_target_properties(arrow_flight_server PROPERTIES
    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin
)

# Print build information
message(STATUS "Building ArrowFlight Relay Server")
message(STATUS "Arrow version: ${Arrow_VERSION}")
message(STATUS "Build type: ${CMAKE_BUILD_TYPE}")

# Installation
install(TARGETS arrow_flight_server
    RUNTIME DESTINATION bin
)

# Create a simple script to run the server
configure_file(
    "${CMAKE_SOURCE_DIR}/run_server.sh.in"
    "${CMAKE_BINARY_DIR}/run_server.sh"
    @ONLY
)

# Make the script executable
if(UNIX)
    execute_process(
        COMMAND chmod +x "${CMAKE_BINARY_DIR}/run_server.sh"
    )
endif()

#!/usr/bin/env python3
"""
Examples demonstrating how different Python processes can communicate
through the ArrowFlight relay server.
"""

import pandas as pd
import numpy as np
import time
import multiprocessing as mp
import threading
import argparse
import sys
from typing import Optional

# Import our client (assuming it's in the same directory)
from arrow_flight_client import ArrowFlightClient, ArrowFlightDataExchange


def data_producer_process(process_id: str, num_batches: int = 5, delay: float = 2.0):
    """
    Simulates a data producer process that generates and sends data periodically.
    """
    print(f"🏭 Producer {process_id} starting...")
    
    try:
        # Initialize client
        client = ArrowFlightClient(sender_id=f"producer_{process_id}")
        
        for i in range(num_batches):
            # Generate sample data
            data = pd.DataFrame({
                'batch_id': [i] * 100,
                'timestamp': pd.date_range('2024-01-01', periods=100, freq='1s'),
                'sensor_value': np.random.randn(100) * 10 + 50,
                'sensor_id': np.random.choice(['sensor_A', 'sensor_B', 'sensor_C'], 100),
                'producer_id': [process_id] * 100
            })
            
            # Send data with a unique key
            key = f"data_batch_{process_id}_{i}"
            success = client.send_dataframe(
                key, 
                data, 
                f"Batch {i} from producer {process_id}"
            )
            
            if success:
                print(f"🟢 Producer {process_id} sent batch {i}")
            else:
                print(f"🔴 Producer {process_id} failed to send batch {i}")
            
            # Send data to a shared channel as well
            shared_key = f"live_feed"
            client.send_dataframe(
                shared_key,
                data,
                f"Live data from {process_id} batch {i}"
            )
            
            time.sleep(delay)
        
        print(f"✅ Producer {process_id} completed")
        
    except Exception as e:
        print(f"❌ Producer {process_id} error: {e}")


def data_consumer_process(process_id: str, keys_to_watch: list, timeout: float = 30.0):
    """
    Simulates a data consumer process that retrieves and processes data.
    """
    print(f"🔍 Consumer {process_id} starting...")
    
    try:
        # Initialize client
        client = ArrowFlightClient(sender_id=f"consumer_{process_id}")
        
        for key in keys_to_watch:
            print(f"   Waiting for data: {key}")
            
            # Wait for data to become available
            if client.wait_for_table(key, timeout):
                # Retrieve and process data
                df = client.get_dataframe(key)
                if df is not None:
                    # Simple processing: calculate statistics
                    stats = {
                        'key': key,
                        'rows': len(df),
                        'mean_sensor_value': df['sensor_value'].mean() if 'sensor_value' in df.columns else None,
                        'unique_sensors': df['sensor_id'].nunique() if 'sensor_id' in df.columns else None
                    }
                    
                    print(f"🟢 Consumer {process_id} processed {key}: {stats}")
                    
                    # Optionally save processed results back to server
                    result_key = f"processed_{key}_{process_id}"
                    result_df = pd.DataFrame([stats])
                    client.send_dataframe(
                        result_key,
                        result_df,
                        f"Processed results by consumer {process_id}"
                    )
                else:
                    print(f"🔴 Consumer {process_id} failed to retrieve {key}")
            else:
                print(f"⏰ Consumer {process_id} timeout waiting for {key}")
        
        print(f"✅ Consumer {process_id} completed")
        
    except Exception as e:
        print(f"❌ Consumer {process_id} error: {e}")


def live_monitor_process(monitor_id: str, channel: str = "live_feed", duration: float = 20.0):
    """
    Simulates a monitoring process that continuously watches for live data.
    """
    print(f"📊 Monitor {monitor_id} starting (watching {channel})...")
    
    try:
        # Initialize data exchange
        exchange = ArrowFlightDataExchange(process_id=f"monitor_{monitor_id}")
        
        start_time = time.time()
        last_data_time = {}
        
        while time.time() - start_time < duration:
            # Check for new data
            data = exchange.subscribe(channel, as_dataframe=True)
            
            if data is not None:
                # Check if this is new data
                data_hash = str(hash(str(data.values.tobytes())))
                current_time = time.time()
                
                if data_hash not in last_data_time:
                    last_data_time[data_hash] = current_time
                    
                    # Process live data
                    latest_value = data['sensor_value'].iloc[-1] if len(data) > 0 else None
                    producer = data['producer_id'].iloc[0] if len(data) > 0 else 'unknown'
                    
                    print(f"🔴 Monitor {monitor_id} - Live data from {producer}: "
                          f"latest value = {latest_value:.2f}")
                    
                    # Save monitoring results
                    monitoring_data = pd.DataFrame({
                        'monitor_time': [current_time],
                        'data_source': [producer],
                        'latest_sensor_value': [latest_value],
                        'data_points': [len(data)]
                    })
                    
                    exchange.publish(
                        f"monitoring_results_{monitor_id}",
                        monitoring_data,
                        f"Monitoring results from {monitor_id}"
                    )
            
            time.sleep(1.0)  # Check every second
        
        print(f"✅ Monitor {monitor_id} completed")
        
    except Exception as e:
        print(f"❌ Monitor {monitor_id} error: {e}")


def aggregator_process(aggregator_id: str, wait_time: float = 15.0):
    """
    Simulates an aggregator process that collects data from multiple sources.
    """
    print(f"📈 Aggregator {aggregator_id} starting...")
    
    try:
        # Initialize client
        client = ArrowFlightClient(sender_id=f"aggregator_{aggregator_id}")
        
        # Wait for some data to accumulate
        time.sleep(wait_time)
        
        # Get all available tables
        tables = client.list_tables()
        print(f"   Found {len(tables)} tables on server")
        
        # Collect all data that matches our pattern
        all_data = []
        for table_info in tables:
            key = table_info['key']
            if 'data_batch_' in key:  # Only process batch data
                df = client.get_dataframe(key)
                if df is not None:
                    all_data.append(df)
        
        if all_data:
            # Aggregate all data
            combined_df = pd.concat(all_data, ignore_index=True)
            
            # Calculate aggregate statistics
            aggregated_stats = pd.DataFrame({
                'total_rows': [len(combined_df)],
                'unique_producers': [combined_df['producer_id'].nunique()],
                'mean_sensor_value': [combined_df['sensor_value'].mean()],
                'std_sensor_value': [combined_df['sensor_value'].std()],
                'min_sensor_value': [combined_df['sensor_value'].min()],
                'max_sensor_value': [combined_df['sensor_value'].max()],
                'aggregation_time': [time.time()]
            })
            
            # Send aggregated results
            success = client.send_dataframe(
                f"aggregated_results_{aggregator_id}",
                aggregated_stats,
                f"Aggregated statistics by {aggregator_id}"
            )
            
            if success:
                print(f"🟢 Aggregator {aggregator_id} processed {len(all_data)} datasets")
                print(f"   Total rows: {len(combined_df)}")
                print(f"   Mean sensor value: {combined_df['sensor_value'].mean():.2f}")
            else:
                print(f"🔴 Aggregator {aggregator_id} failed to send results")
        else:
            print(f"⚠️  Aggregator {aggregator_id} found no data to process")
        
        print(f"✅ Aggregator {aggregator_id} completed")
        
    except Exception as e:
        print(f"❌ Aggregator {aggregator_id} error: {e}")


def run_multi_process_example():
    """
    Run a complete multi-process example demonstrating the relay server.
    """
    print("🚀 Starting Multi-Process ArrowFlight Relay Example")
    print("=" * 60)
    
    # Test server connection first
    try:
        test_client = ArrowFlightClient()
        print("✅ Server connection successful")
    except Exception as e:
        print(f"❌ Cannot connect to server: {e}")
        print("   Make sure the C++ ArrowFlight server is running!")
        return
    
    # Clear any existing data
    test_client.clear_server()
    print("🧹 Cleared existing data from server")
    
    processes = []
    
    try:
        # Start producer processes
        for i in range(2):
            p = mp.Process(
                target=data_producer_process,
                args=(f"P{i}", 3, 2.0)  # 3 batches, 2 second delay
            )
            p.start()
            processes.append(p)
        
        # Start monitor process
        monitor_p = mp.Process(
            target=live_monitor_process,
            args=("M1", "live_feed", 15.0)  # Monitor for 15 seconds
        )
        monitor_p.start()
        processes.append(monitor_p)
        
        # Start consumer processes (they will wait for specific data)
        keys_to_consume = [
            ["data_batch_P0_0", "data_batch_P0_1"],
            ["data_batch_P1_0", "data_batch_P1_2"]
        ]
        
        for i, keys in enumerate(keys_to_consume):
            p = mp.Process(
                target=data_consumer_process,
                args=(f"C{i}", keys, 30.0)
            )
            p.start()
            processes.append(p)
        
        # Start aggregator process
        agg_p = mp.Process(
            target=aggregator_process,
            args=("A1", 10.0)  # Wait 10 seconds before aggregating
        )
        agg_p.start()
        processes.append(agg_p)
        
        # Wait for all processes to complete
        for p in processes:
            p.join()
        
        print("\n📊 Final Server Statistics:")
        print("=" * 40)
        stats = test_client.get_server_stats()
        print(f"Total stored tables: {stats.get('stored_tables', 'unknown')}")
        
        for table_info in stats.get('tables', []):
            print(f"  • {table_info['key']}: {table_info['rows']} rows, "
                  f"age {table_info['age_seconds']}s")
        
        print("\n🎉 Multi-process example completed successfully!")
        
    except KeyboardInterrupt:
        print("\n⚠️  Interrupted by user")
        for p in processes:
            if p.is_alive():
                p.terminate()


def run_simple_example():
    """
    Run a simple single-process example.
    """
    print("🚀 Starting Simple ArrowFlight Example")
    print("=" * 40)
    
    try:
        # Initialize data exchange
        exchange = ArrowFlightDataExchange(process_id="simple_example")
        
        # Create sample data
        sample_data = pd.DataFrame({
            'id': range(100),
            'value': np.random.randn(100),
            'category': np.random.choice(['A', 'B', 'C'], 100)
        })
        
        # Publish data
        print("📤 Publishing sample data...")
        success = exchange.publish("simple_channel", sample_data, "Simple example data")
        print(f"   Result: {'✅ Success' if success else '❌ Failed'}")
        
        # Subscribe to data
        print("📥 Subscribing to data...")
        retrieved_data = exchange.subscribe("simple_channel")
        
        if retrieved_data is not None:
            print(f"   ✅ Retrieved {len(retrieved_data)} rows")
            print(f"   Sample: {retrieved_data.head()}")
        else:
            print("   ❌ Failed to retrieve data")
        
        # List channels
        channels = exchange.list_channels()
        print(f"📋 Available channels: {channels}")
        
        # Get server status
        status = exchange.get_server_status()
        print(f"📊 Server has {status.get('stored_tables', 0)} stored tables")
        
        print("🎉 Simple example completed!")
        
    except Exception as e:
        print(f"❌ Error: {e}")


def main():
    """
    Main function with command line argument parsing.
    """
    parser = argparse.ArgumentParser(description="ArrowFlight Relay Examples")
    parser.add_argument(
        "--mode",
        choices=["simple", "multi", "producer", "consumer", "monitor", "aggregator"],
        default="simple",
        help="Example mode to run"
    )
    parser.add_argument("--id", default="1", help="Process ID")
    parser.add_argument("--keys", nargs="*", help="Keys to watch (for consumer)")
    parser.add_argument("--channel", default="live_feed", help="Channel to monitor")
    parser.add_argument("--duration", type=float, default=20.0, help="Duration to run")
    
    args = parser.parse_args()
    
    if args.mode == "simple":
        run_simple_example()
    elif args.mode == "multi":
        run_multi_process_example()
    elif args.mode == "producer":
        data_producer_process(args.id)
    elif args.mode == "consumer":
        keys = args.keys or [f"data_batch_{args.id}_0"]
        data_consumer_process(args.id, keys)
    elif args.mode == "monitor":
        live_monitor_process(args.id, args.channel, args.duration)
    elif args.mode == "aggregator":
        aggregator_process(args.id)
    else:
        print(f"Unknown mode: {args.mode}")
        sys.exit(1)


if __name__ == "__main__":
    main()
