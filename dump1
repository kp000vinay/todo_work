#!/usr/bin/env python3
"""
Examples demonstrating how different Python processes can communicate
through the ArrowFlight relay server.
"""

import pandas as pd
import numpy as np
import time
import multiprocessing as mp
import threading
import argparse
import sys
from typing import Optional

# Import our client (assuming it's in the same directory)
from arrow_flight_client import ArrowFlightClient, ArrowFlightDataExchange


def data_producer_process(process_id: str, num_batches: int = 5, delay: float = 2.0):
    """
    Simulates a data producer process that generates and sends data periodically.
    """
    print(f"üè≠ Producer {process_id} starting...")
    
    try:
        # Initialize client
        client = ArrowFlightClient(sender_id=f"producer_{process_id}")
        
        for i in range(num_batches):
            # Generate sample data
            data = pd.DataFrame({
                'batch_id': [i] * 100,
                'timestamp': pd.date_range('2024-01-01', periods=100, freq='1s'),
                'sensor_value': np.random.randn(100) * 10 + 50,
                'sensor_id': np.random.choice(['sensor_A', 'sensor_B', 'sensor_C'], 100),
                'producer_id': [process_id] * 100
            })
            
            # Send data with a unique key
            key = f"data_batch_{process_id}_{i}"
            success = client.send_dataframe(
                key, 
                data, 
                f"Batch {i} from producer {process_id}"
            )
            
            if success:
                print(f"üü¢ Producer {process_id} sent batch {i}")
            else:
                print(f"üî¥ Producer {process_id} failed to send batch {i}")
            
            # Send data to a shared channel as well
            shared_key = f"live_feed"
            client.send_dataframe(
                shared_key,
                data,
                f"Live data from {process_id} batch {i}"
            )
            
            time.sleep(delay)
        
        print(f"‚úÖ Producer {process_id} completed")
        
    except Exception as e:
        print(f"‚ùå Producer {process_id} error: {e}")


def data_consumer_process(process_id: str, keys_to_watch: list, timeout: float = 30.0):
    """
    Simulates a data consumer process that retrieves and processes data.
    """
    print(f"üîç Consumer {process_id} starting...")
    
    try:
        # Initialize client
        client = ArrowFlightClient(sender_id=f"consumer_{process_id}")
        
        for key in keys_to_watch:
            print(f"   Waiting for data: {key}")
            
            # Wait for data to become available
            if client.wait_for_table(key, timeout):
                # Retrieve and process data
                df = client.get_dataframe(key)
                if df is not None:
                    # Simple processing: calculate statistics
                    stats = {
                        'key': key,
                        'rows': len(df),
                        'mean_sensor_value': df['sensor_value'].mean() if 'sensor_value' in df.columns else None,
                        'unique_sensors': df['sensor_id'].nunique() if 'sensor_id' in df.columns else None
                    }
                    
                    print(f"üü¢ Consumer {process_id} processed {key}: {stats}")
                    
                    # Optionally save processed results back to server
                    result_key = f"processed_{key}_{process_id}"
                    result_df = pd.DataFrame([stats])
                    client.send_dataframe(
                        result_key,
                        result_df,
                        f"Processed results by consumer {process_id}"
                    )
                else:
                    print(f"üî¥ Consumer {process_id} failed to retrieve {key}")
            else:
                print(f"‚è∞ Consumer {process_id} timeout waiting for {key}")
        
        print(f"‚úÖ Consumer {process_id} completed")
        
    except Exception as e:
        print(f"‚ùå Consumer {process_id} error: {e}")


def live_monitor_process(monitor_id: str, channels: list = None, duration: float = 20.0, 
                        poll_interval: float = 1.0):
    """
    Advanced monitoring process that watches multiple channels for live data.
    """
    if channels is None:
        channels = ["live_feed"]
    
    print(f"üìä Monitor {monitor_id} starting...")
    print(f"   Watching channels: {channels}")
    print(f"   Duration: {duration}s, Poll interval: {poll_interval}s")
    
    try:
        # Initialize data exchange
        exchange = ArrowFlightDataExchange(process_id=f"monitor_{monitor_id}")
        
        start_time = time.time()
        last_seen_data = {}  # Track last seen data per channel
        monitoring_stats = {
            'total_updates': 0,
            'channels_seen': set(),
            'producers_seen': set(),
            'first_data_time': None,
            'last_data_time': None
        }
        
        print(f"üü¢ Monitor {monitor_id} active - watching for data...")
        
        while time.time() - start_time < duration:
            current_time = time.time()
            elapsed_time = current_time - start_time
            
            # Check each channel
            for channel in channels:
                try:
                    # Get current data
                    data = exchange.subscribe(channel, as_dataframe=True)
                    
                    if data is not None and len(data) > 0:
                        # Create a unique identifier for this data
                        data_signature = {
                            'shape': data.shape,
                            'columns': list(data.columns),
                            'first_row_hash': hash(str(data.iloc[0].to_dict())) if len(data) > 0 else None,
                            'last_row_hash': hash(str(data.iloc[-1].to_dict())) if len(data) > 0 else None
                        }
                        
                        # Check if this is new data
                        last_signature = last_seen_data.get(channel)
                        is_new_data = (last_signature is None or 
                                     last_signature != data_signature)
                        
                        if is_new_data:
                            last_seen_data[channel] = data_signature
                            monitoring_stats['total_updates'] += 1
                            monitoring_stats['channels_seen'].add(channel)
                            
                            if monitoring_stats['first_data_time'] is None:
                                monitoring_stats['first_data_time'] = current_time
                            monitoring_stats['last_data_time'] = current_time
                            
                            # Extract metadata from data
                            producer = 'unknown'
                            latest_value = None
                            data_timestamp = None
                            
                            if 'producer_id' in data.columns:
                                producer = data['producer_id'].iloc[0]
                                monitoring_stats['producers_seen'].add(producer)
                            
                            if 'sensor_value' in data.columns:
                                latest_value = data['sensor_value'].iloc[-1]
                            
                            if 'timestamp' in data.columns:
                                data_timestamp = data['timestamp'].iloc[-1]
                            
                            # Log the update
                            print(f"üî¥ [{elapsed_time:.1f}s] Monitor {monitor_id} - "
                                  f"New data on '{channel}':")
                            print(f"     Producer: {producer}")
                            print(f"     Shape: {data.shape}")
                            print(f"     Latest value: {latest_value}")
                            if data_timestamp:
                                print(f"     Data timestamp: {data_timestamp}")
                            
                            # Create detailed monitoring record
                            monitoring_record = pd.DataFrame({
                                'monitor_id': [monitor_id],
                                'monitor_time': [current_time],
                                'elapsed_time': [elapsed_time],
                                'channel': [channel],
                                'data_source': [producer],
                                'data_shape_rows': [data.shape[0]],
                                'data_shape_cols': [data.shape[1]],
                                'latest_sensor_value': [latest_value],
                                'data_timestamp': [str(data_timestamp) if data_timestamp else None],
                                'columns': [','.join(data.columns)],
                                'update_sequence': [monitoring_stats['total_updates']]
                            })
                            
                            # Save monitoring results
                            monitor_key = f"monitor_log_{monitor_id}_{monitoring_stats['total_updates']:03d}"
                            exchange.publish(
                                monitor_key,
                                monitoring_record,
                                f"Monitor {monitor_id} data update #{monitoring_stats['total_updates']}"
                            )
                            
                            # Also save to a continuous log
                            exchange.publish(
                                f"monitor_continuous_log_{monitor_id}",
                                monitoring_record,
                                f"Continuous monitoring log from {monitor_id}"
                            )
                
                except Exception as e:
                    print(f"‚ö†Ô∏è  Monitor {monitor_id} error checking channel '{channel}': {e}")
            
            # Periodic status update
            if int(elapsed_time) % 5 == 0 and elapsed_time > 0:
                print(f"üìä [{elapsed_time:.0f}s] Monitor {monitor_id} status: "
                      f"{monitoring_stats['total_updates']} updates, "
                      f"{len(monitoring_stats['channels_seen'])} channels active, "
                      f"{len(monitoring_stats['producers_seen'])} producers seen")
            
            time.sleep(poll_interval)
        
        # Final summary
        total_time = time.time() - start_time
        print(f"\n‚úÖ Monitor {monitor_id} completed after {total_time:.1f}s")
        print(f"   üìà Summary:")
        print(f"     Total data updates detected: {monitoring_stats['total_updates']}")
        print(f"     Active channels: {list(monitoring_stats['channels_seen'])}")
        print(f"     Producers seen: {list(monitoring_stats['producers_seen'])}")
        
        if monitoring_stats['first_data_time']:
            data_active_time = monitoring_stats['last_data_time'] - monitoring_stats['first_data_time']
            print(f"     Data activity period: {data_active_time:.1f}s")
            print(f"     Average update rate: {monitoring_stats['total_updates']/data_active_time:.2f} updates/sec")
        
        # Save final summary
        summary_data = pd.DataFrame({
            'monitor_id': [monitor_id],
            'total_runtime': [total_time],
            'total_updates': [monitoring_stats['total_updates']],
            'channels_monitored': [','.join(channels)],
            'channels_active': [','.join(monitoring_stats['channels_seen'])],
            'producers_seen': [','.join(monitoring_stats['producers_seen'])],
            'completion_time': [time.time()]
        })
        
        exchange.publish(
            f"monitor_summary_{monitor_id}",
            summary_data,
            f"Final monitoring summary from {monitor_id}"
        )
        
    except Exception as e:
        print(f"‚ùå Monitor {monitor_id} error: {e}")


def aggregator_process(aggregator_id: str, wait_time: float = 15.0):
    """
    Simulates an aggregator process that collects data from multiple sources.
    """
    print(f"üìà Aggregator {aggregator_id} starting...")
    
    try:
        # Initialize client
        client = ArrowFlightClient(sender_id=f"aggregator_{aggregator_id}")
        
        # Wait for some data to accumulate
        time.sleep(wait_time)
        
        # Get all available tables
        tables = client.list_tables()
        print(f"   Found {len(tables)} tables on server")
        
        # Collect all data that matches our pattern
        all_data = []
        for table_info in tables:
            key = table_info['key']
            if 'data_batch_' in key:  # Only process batch data
                df = client.get_dataframe(key)
                if df is not None:
                    all_data.append(df)
        
        if all_data:
            # Aggregate all data
            combined_df = pd.concat(all_data, ignore_index=True)
            
            # Calculate aggregate statistics
            aggregated_stats = pd.DataFrame({
                'total_rows': [len(combined_df)],
                'unique_producers': [combined_df['producer_id'].nunique()],
                'mean_sensor_value': [combined_df['sensor_value'].mean()],
                'std_sensor_value': [combined_df['sensor_value'].std()],
                'min_sensor_value': [combined_df['sensor_value'].min()],
                'max_sensor_value': [combined_df['sensor_value'].max()],
                'aggregation_time': [time.time()]
            })
            
            # Send aggregated results
            success = client.send_dataframe(
                f"aggregated_results_{aggregator_id}",
                aggregated_stats,
                f"Aggregated statistics by {aggregator_id}"
            )
            
            if success:
                print(f"üü¢ Aggregator {aggregator_id} processed {len(all_data)} datasets")
                print(f"   Total rows: {len(combined_df)}")
                print(f"   Mean sensor value: {combined_df['sensor_value'].mean():.2f}")
            else:
                print(f"üî¥ Aggregator {aggregator_id} failed to send results")
        else:
            print(f"‚ö†Ô∏è  Aggregator {aggregator_id} found no data to process")
        
        print(f"‚úÖ Aggregator {aggregator_id} completed")
        
    except Exception as e:
        print(f"‚ùå Aggregator {aggregator_id} error: {e}")


def run_multi_process_example():
    """
    Run a complete multi-process example demonstrating the relay server.
    """
    print("üöÄ Starting Multi-Process ArrowFlight Relay Example")
    print("=" * 60)
    
    # Test server connection first
    try:
        test_client = ArrowFlightClient()
        print("‚úÖ Server connection successful")
    except Exception as e:
        print(f"‚ùå Cannot connect to server: {e}")
        print("   Make sure the C++ ArrowFlight server is running!")
        return
    
    # Clear any existing data
    test_client.clear_server()
    print("üßπ Cleared existing data from server")
    
    processes = []
    
    try:
        # Start producer processes
        for i in range(2):
            p = mp.Process(
                target=data_producer_process,
                args=(f"P{i}", 3, 2.0)  # 3 batches, 2 second delay
            )
            p.start()
            processes.append(p)
        
        # Start monitor process
        monitor_p = mp.Process(
            target=live_monitor_process,
            args=("M1", ["live_feed", "data_batch_P0_0"], 15.0, 1.0)  # Monitor multiple channels
        )
        monitor_p.start()
        processes.append(monitor_p)
        
        # Start consumer processes (they will wait for specific data)
        keys_to_consume = [
            ["data_batch_P0_0", "data_batch_P0_1"],
            ["data_batch_P1_0", "data_batch_P1_2"]
        ]
        
        for i, keys in enumerate(keys_to_consume):
            p = mp.Process(
                target=data_consumer_process,
                args=(f"C{i}", keys, 30.0)
            )
            p.start()
            processes.append(p)
        
        # Start aggregator process
        agg_p = mp.Process(
            target=aggregator_process,
            args=("A1", 10.0)  # Wait 10 seconds before aggregating
        )
        agg_p.start()
        processes.append(agg_p)
        
        # Wait for all processes to complete
        for p in processes:
            p.join()
        
        print("\nüìä Final Server Statistics:")
        print("=" * 40)
        stats = test_client.get_server_stats()
        print(f"Total stored tables: {stats.get('stored_tables', 'unknown')}")
        
        for table_info in stats.get('tables', []):
            print(f"  ‚Ä¢ {table_info['key']}: {table_info['rows']} rows, "
                  f"age {table_info['age_seconds']}s")
        
        print("\nüéâ Multi-process example completed successfully!")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Interrupted by user")
        for p in processes:
            if p.is_alive():
                p.terminate()


def run_advanced_monitoring_example():
    """
    Demonstrate advanced monitoring capabilities.
    """
    print("üîç Starting Advanced Monitoring Example")
    print("=" * 50)
    
    try:
        # Test server connection
        test_client = ArrowFlightClient()
        print("‚úÖ Server connection successful")
        
        # Clear existing data
        test_client.clear_server()
        print("üßπ Cleared existing data")
        
        processes = []
        
        # Start multiple producers with different patterns
        producer_configs = [
            ("FastProducer", 5, 0.5),    # 5 batches, 0.5s delay
            ("SlowProducer", 3, 2.0),    # 3 batches, 2s delay
            ("BurstProducer", 8, 0.2),   # 8 batches, 0.2s delay
        ]
        
        for prod_id, batches, delay in producer_configs:
            p = mp.Process(
                target=data_producer_process,
                args=(prod_id, batches, delay)
            )
            p.start()
            processes.append(p)
        
        # Start advanced monitor watching multiple channels
        monitor_channels = [
            "live_feed",
            "data_batch_FastProducer_0",
            "data_batch_SlowProducer_0",
            "data_batch_BurstProducer_0"
        ]
        
        monitor_p = mp.Process(
            target=live_monitor_process,
            args=("AdvancedMonitor", monitor_channels, 12.0, 0.5)  # 12s duration, 0.5s poll
        )
        monitor_p.start()
        processes.append(monitor_p)
        
        # Wait for all processes
        for p in processes:
            p.join()
        
        # Analyze monitoring results
        print("\nüìä Analyzing monitoring results...")
        time.sleep(1)  # Give time for final writes
        
        tables = test_client.list_tables()
        monitor_tables = [t for t in tables if 'monitor' in t['key']]
        
        print(f"üìã Found {len(monitor_tables)} monitoring result tables:")
        for table in monitor_tables:
            print(f"   ‚Ä¢ {table['key']}: {table['total_records']} records")
        
        # Get and display summary
        summary_data = test_client.get_dataframe("monitor_summary_AdvancedMonitor")
        if summary_data is not None:
            print(f"\nüìà Monitor Summary:")
            print(f"   Runtime: {summary_data['total_runtime'].iloc[0]:.1f}s")
            print(f"   Updates detected: {summary_data['total_updates'].iloc[0]}")
            print(f"   Channels monitored: {summary_data['channels_monitored'].iloc[0]}")
            print(f"   Active channels: {summary_data['channels_active'].iloc[0]}")
            print(f"   Producers seen: {summary_data['producers_seen'].iloc[0]}")
        
        print("\nüéâ Advanced monitoring example completed!")
        
    except Exception as e:
        print(f"‚ùå Error in advanced monitoring example: {e}")
        for p in processes:
            if p.is_alive():
                p.terminate()


def run_simple_example():
    """
    Run a simple single-process example.
    """
    print("üöÄ Starting Simple ArrowFlight Example")
    print("=" * 40)
    
    try:
        # Initialize data exchange
        exchange = ArrowFlightDataExchange(process_id="simple_example")
        
        # Create sample data
        sample_data = pd.DataFrame({
            'id': range(100),
            'value': np.random.randn(100),
            'category': np.random.choice(['A', 'B', 'C'], 100)
        })
        
        # Publish data
        print("üì§ Publishing sample data...")
        success = exchange.publish("simple_channel", sample_data, "Simple example data")
        print(f"   Result: {'‚úÖ Success' if success else '‚ùå Failed'}")
        
        # Subscribe to data
        print("üì• Subscribing to data...")
        retrieved_data = exchange.subscribe("simple_channel")
        
        if retrieved_data is not None:
            print(f"   ‚úÖ Retrieved {len(retrieved_data)} rows")
            print(f"   Sample: {retrieved_data.head()}")
        else:
            print("   ‚ùå Failed to retrieve data")
        
        # List channels
        channels = exchange.list_channels()
        print(f"üìã Available channels: {channels}")
        
        # Get server status
        status = exchange.get_server_status()
        print(f"üìä Server has {status.get('stored_tables', 0)} stored tables")
        
        print("üéâ Simple example completed!")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
    """
    Run a simple single-process example.
    """
    print("üöÄ Starting Simple ArrowFlight Example")
    print("=" * 40)
    
    try:
        # Initialize data exchange
        exchange = ArrowFlightDataExchange(process_id="simple_example")
        
        # Create sample data
        sample_data = pd.DataFrame({
            'id': range(100),
            'value': np.random.randn(100),
            'category': np.random.choice(['A', 'B', 'C'], 100)
        })
        
        # Publish data
        print("üì§ Publishing sample data...")
        success = exchange.publish("simple_channel", sample_data, "Simple example data")
        print(f"   Result: {'‚úÖ Success' if success else '‚ùå Failed'}")
        
        # Subscribe to data
        print("üì• Subscribing to data...")
        retrieved_data = exchange.subscribe("simple_channel")
        
        if retrieved_data is not None:
            print(f"   ‚úÖ Retrieved {len(retrieved_data)} rows")
            print(f"   Sample: {retrieved_data.head()}")
        else:
            print("   ‚ùå Failed to retrieve data")
        
        # List channels
        channels = exchange.list_channels()
        print(f"üìã Available channels: {channels}")
        
        # Get server status
        status = exchange.get_server_status()
        print(f"üìä Server has {status.get('stored_tables', 0)} stored tables")
        
        print("üéâ Simple example completed!")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")


def main():
    """
    Main function with command line argument parsing.
    """
    parser = argparse.ArgumentParser(description="ArrowFlight Relay Examples")
    parser.add_argument(
        "--mode",
        choices=["simple", "multi", "advanced-monitor", "producer", "consumer", "monitor", "aggregator"],
        default="simple",
        help="Example mode to run"
    )
    parser.add_argument("--id", default="1", help="Process ID")
    parser.add_argument("--keys", nargs="*", help="Keys to watch (for consumer)")
    parser.add_argument("--channels", nargs="*", help="Channels to monitor (for monitor mode)")
    parser.add_argument("--channel", default="live_feed", help="Single channel to monitor (deprecated, use --channels)")
    parser.add_argument("--duration", type=float, default=20.0, help="Duration to run")
    parser.add_argument("--poll-interval", type=float, default=1.0, help="Polling interval for monitor")
    
    args = parser.parse_args()
    
    if args.mode == "simple":
        run_simple_example()
    elif args.mode == "multi":
        run_multi_process_example()
    elif args.mode == "advanced-monitor":
        run_advanced_monitoring_example()
    elif args.mode == "producer":
        data_producer_process(args.id)
    elif args.mode == "consumer":
        keys = args.keys or [f"data_batch_{args.id}_0"]
        data_consumer_process(args.id, keys)
    elif args.mode == "monitor":
        channels = args.channels or [args.channel]  # Use new channels parameter or fall back to single channel
        live_monitor_process(args.id, channels, args.duration, args.poll_interval)
    elif args.mode == "aggregator":
        aggregator_process(args.id)
    else:
        print(f"Unknown mode: {args.mode}")
        sys.exit(1)


if __name__ == "__main__":
    main()
